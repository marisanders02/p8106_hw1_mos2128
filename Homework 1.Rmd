---
title: "Homework 1"
author: "Mari Sanders"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(caret)
library(tidyverse)
library(plotmo) 
```

# A) 

```{r}
training_dat <- read_csv("housing_training.csv")
test <- read_csv("housing_test.csv")
```

```{r}
set.seed(10)
x <- model.matrix(Sale_Price ~ ., training_dat)[,-1]
y <- training_dat$Sale_Price
x2 <- model.matrix(Sale_Price ~ .,test)[, -1] 
y2 <- test$Sale_Price
```

```{r}
set.seed(10)
ctrl1 <- trainControl(method = "cv", number = 10)
lasso.fit <- train(x,y,
                   data = training_dat,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(6, 0, length = 100))),
                   trControl = ctrl1)

lasso.fit$bestTune

lasso_predict <- predict(lasso.fit,newdata = model.matrix(Sale_Price ~ ., test)[,-1])
mse_lasso <- mean((lasso_predict - test$Sale_Price)^2)
rmse_lasso <- sqrt(mean((lasso_predict - test$Sale_Price)^2))

lasso_coef <- coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)
```

The lasso model fit with the best tune parameter has 23 parameters. The error is `r mse_lasso` and the rmse is `r rmse_lasso`. 

```{r}
set.seed(10)
minrmse <- min(lasso.fit$results$RMSE)
sdrmse <- sd(lasso.fit$results$RMSE)
rmse_1se <- minrmse + sdrmse

lambda_1se <- max(lasso.fit$results$lambda[lasso.fit$results$RMSE <= rmse_1se])

lasso.fit1se <- train(x,y, 
                   data = training_dat, 
                   method = "glmnet", 
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = lambda_1se),
                   trControl = ctrl1)
lasso.fit1se$bestTune

lasso_predict1se <- predict(lasso.fit1se,newdata = model.matrix(Sale_Price ~ ., test)[,-1])


mse_lasso1se <- mean((lasso_predict - test$Sale_Price)^2)
rmse_lasso1se <- sqrt(mean((lasso_predict - test$Sale_Price)^2))

lasso_coef1se <- coef(lasso.fit1se$finalModel, s = lasso.fit1se$bestTune$lambda)
```

The lasso model fit with the 1se paramter has 23 predictors. The error is `r mse_lasso1se` and the rmse is `r rmse_lasso1se`. 

# B)

Yes 1se is applicable here because cross-validation results provide the error curves for multiple values of lambda. The 1SE rule selects the most regularized model within one standard error of the minimum cross-validation error, favoring simpler models.

```{r}
set.seed(10)
ctrl1 <- trainControl(method = "cv", number = 10)
enet.fit <- train(x, y,
                  data = training_dat,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(6,0, length = 100))),
                  trControl = ctrl1)
enet.fit$bestTune

enet_pred <-  predict(enet.fit, 
                      newdata = model.matrix(Sale_Price ~ ., test)[,-1])
enet_coef <- coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

mse_enet <- mean((enet_pred - test$Sale_Price)^2)
rmse_enet <- sqrt(mean((enet_pred - test$Sale_Price)^2))

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)


```

The tuning parameter for elastic net with minimum lambda is `r enet.fit$bestTune`. The model kept 23 parameters. The mse is `r mse_enet` and the rmse is `r rmse_enet`. 

```{r}
minrmse <- min(enet.fit$results$RMSE)
sdrmse <- sd(enet.fit$results$RMSE)

rmse_1se <- minrmse + sdrmse

lambda_1se <- max(enet.fit$results$lambda[enet.fit$results$RMSE <= rmse_1se])
set.seed(10)
enet.fit_1se <- train(x, y,
                  data = training_dat,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = enet.fit$bestTune$alpha, 
                                         lambda = lambda_1se),
                  trControl = ctrl1)

enet_1se_pred <-  predict(enet.fit_1se, 
                          newdata = model.matrix(Sale_Price ~ ., test)[,-1])

mse_enet1se <- mean((enet_1se_pred - test$Sale_Price)^2)
rmse_enet1se <- sqrt(mean((enet_1se_pred - test$Sale_Price)^2))

enet_coef1se <- coef(enet.fit_1se$finalModel, s = enet.fit_1se$bestTune$lambda)

```

The tuning parameter for elastic net with1SE RULE is `r enet.fit_1se$bestTune`. The model kept 23 parameters. The mse is `r mse_enet1se` and the rmse is `r rmse_enet1se`. 

# C)

```{r}
set.seed(10)
pls_fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:25),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))
predy2_pls2 <- predict(pls_fit, newdata = x2)

coefficients <- coef(pls_fit$finalModel, ncomp = pls_fit$bestTune$ncomp)

mse_pls <- mean((y2 - predy2_pls2)^2)

rmse_pls <- sqrt(mean((y2 - predy2_pls2)^2))


ggplot(pls_fit, highlight = TRUE)
```

There are 12 components included in this model.  The mse is `r mse_pls` and the rmse is `r rmse_pls`


# D) 

```{r}
set.seed(10)
resamp <- resamples(list(enet = enet.fit, enet1se = enet.fit_1se, lasso = lasso.fit, lasso1se = lasso.fit1se))

summary(resamp)


parallelplot(resamp, metric = "RMSE")

bwplot(resamp, metric = "RMSE") 
```

It seems like elastic net is the best model. It has the smallest median rmse when comparing all the models. However, elastic net and lasso are both fairly close in rmse. 

E) 

```{r}
set.seed(10)
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(6, 0, length = 100)))

cv.lasso$lambda.min
lasso.fit$bestTune$lambda

```

Using `glmnet` to train the model, the selected tuning parameter is `r cv.lasso$lambda.min`, while using `caret` got the result that the selected tuning parameter is `r lasso.fit$bestTune$lambda`. They are quite different in their results. 